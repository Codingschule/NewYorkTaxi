{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Yellow Taxi Tripdata\n",
    "### Analyse der New Yorker Yellow Taxi Tripdata\n",
    "In diesem Projekt tauchen wir in die faszinierende Welt der New Yorker Taxis ein. Der Datensatz enthält Millionen von Fahrten der gelben Taxis, inklusive genauer Informationen zu jeder Tour: Wann und wo die Fahrt begann und endete, wie lange sie dauerte, wie viele Passagiere mitfuhren und wie hoch der Fahrpreis war.\n",
    "\n",
    "Unser Ziel ist es, diese umfangreichen Daten zu analysieren, um spannende Muster und Erkenntnisse zu entdecken. Wir werden zum Beispiel herausfinden, zu welchen Zeiten die meisten Taxis unterwegs sind, welche Strecken am beliebtesten sind oder wie sich die Fahrpreise zusammensetzen.\n",
    "\n",
    "Durch die Arbeit mit diesem realen Datensatz lernen wir nicht nur, wie man große Datenmengen verarbeitet, sondern erhalten auch einen tiefen Einblick in das pulsierende Verkehrsnetz von New York City."
   ],
   "id": "e9314c76c620458f"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-11T09:24:01.499737Z",
     "start_time": "2025-08-11T09:24:01.348090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import der\n",
    "import pandas as pd\n",
    "import glob\n",
    "#import pyarrow\n",
    "from fastparquet import ParquetFile\n",
    "#from fastparquet import write"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Preperation\n",
    "#### Warum CSV für diesen Datensatz ungeeignet ist\n",
    "Dieser Datensatz ist mit bis zu 16 GB sehr groß. Das CSV-Format ist für diese Menge an Daten nicht ideal. Da es zeilenorientiert ist und die Komprimierung ineffizient ist, dauert das Einlesen und Speichern viel zu lange. Diese Zeit können wir uns sparen.\n",
    "\n",
    "#### Warum Parquet die bessere Wahl ist\n",
    "Das Parquet-Format ist die perfekte Lösung für solch große Datensätze. Es ist ein modernes, spalten-orientiertes Datenformat. Das bedeutet, dass beim Einlesen nur die Spalten geladen werden, die du wirklich brauchst – das spart enorm viel Zeit und Arbeitsspeicher. Parquet-Dateien sind außerdem hoch komprimiert, was die Dateigröße drastisch reduziert (in unserem Fall von 16 GB auf ca. 3 GB).\n",
    "\n",
    "Parquet ist daher der Standard für Big-Data-Anwendungen und wird auch in Data-Warehouse-Systemen wie AWS Redshift häufig verwendet. Da auch Pandas sehr gut mit Parquet-Dateien umgehen kann, ist das die ideale Arbeitsgrundlage für uns.\n",
    "\n",
    "#### Unser Plan\n",
    "Wir werden die Daten aus den einzelnen CSV-Dateien zunächst mit Pandas einlesen und dann direkt in ein einziges, großes Parquet-File umwandeln. Dieses File wird unsere neue, deutlich effizientere Arbeitsgrundlage für alle weiteren Schritte sein."
   ],
   "id": "f77df8acfc7e991f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T09:26:21.160127Z",
     "start_time": "2025-08-11T09:24:07.729793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Das Muster, das zu allen 12 Dateien passt\n",
    "dateipfad_muster = 'NYTaxi-TripData/yellow_tripdata_*.csv'\n",
    "dateiliste = glob.glob(dateipfad_muster)\n",
    "\n",
    "# Erstelle eine leere Liste, um die DataFrames zu speichern\n",
    "dfs = []\n",
    "\n",
    "# Iteriere durch die Liste der Dateien und lade jede einzelne in eine DataFrame\n",
    "for datei in dateiliste:\n",
    "    #df = pd.read_csv(datei) <- erzeugt Warnungen\n",
    "    # pd.read_csv(datei, low_memory=False) <- vermeidet die Warnungen führt aber zu hohem Speicherverbrauch\n",
    "    df = pd.read_csv(datei, dtype={6: str}) # legt die problematische Spalte auf den Datentyp string fest\n",
    "    dfs.append(df)\n",
    "\n",
    "# Verbinde alle DataFrames zu einer einzigen, großen DataFrame\n",
    "yellow_trips_df = pd.concat(dfs, ignore_index=True)"
   ],
   "id": "1975ddcf8c146af7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T09:26:35.375880Z",
     "start_time": "2025-08-11T09:26:35.306642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ausgeben des Head-Bereichs\n",
    "yellow_trips_df.head()"
   ],
   "id": "25c549bdaaf4c7c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0       1.0  2019-11-01 00:30:41   2019-11-01 00:32:25              1.0   \n",
       "1       1.0  2019-11-01 00:34:01   2019-11-01 00:34:09              1.0   \n",
       "2       2.0  2019-11-01 00:41:59   2019-11-01 00:42:23              1.0   \n",
       "3       2.0  2019-11-01 00:02:39   2019-11-01 00:02:51              1.0   \n",
       "4       2.0  2019-11-01 00:18:30   2019-11-01 00:18:39              2.0   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0            0.0         1.0                  N           145           145   \n",
       "1            0.0         1.0                  N           145           145   \n",
       "2            0.0         1.0                  N           193           193   \n",
       "3            0.0         1.0                  N           193           193   \n",
       "4            0.0         1.0                  N           226           226   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0           2.0          3.0    0.5      0.5        0.00           0.0   \n",
       "1           2.0          2.5    0.5      0.5        0.00           0.0   \n",
       "2           1.0          2.5    0.5      0.5        0.95           0.0   \n",
       "3           1.0          2.5    0.5      0.5        0.95           0.0   \n",
       "4           2.0          2.5    0.0      0.5        0.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  \n",
       "0                    0.3          4.30                   0.0  \n",
       "1                    0.3          3.80                   0.0  \n",
       "2                    0.3          4.75                   0.0  \n",
       "3                    0.3          4.75                   0.0  \n",
       "4                    0.3          3.30                   0.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-11-01 00:30:41</td>\n",
       "      <td>2019-11-01 00:32:25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.30</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-11-01 00:34:01</td>\n",
       "      <td>2019-11-01 00:34:09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-11-01 00:41:59</td>\n",
       "      <td>2019-11-01 00:42:23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.75</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-11-01 00:02:39</td>\n",
       "      <td>2019-11-01 00:02:51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.75</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2019-11-01 00:18:30</td>\n",
       "      <td>2019-11-01 00:18:39</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>226</td>\n",
       "      <td>226</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:14:43.849111Z",
     "start_time": "2025-08-11T08:14:43.833960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ein erster Überblick über die vorhandenen Datentypen\n",
    "# was fällt dir auf?\n",
    "yellow_trips_df.info()"
   ],
   "id": "71055c72dfa5112b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101246797 entries, 0 to 101246796\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   VendorID               float64\n",
      " 1   tpep_pickup_datetime   object \n",
      " 2   tpep_dropoff_datetime  object \n",
      " 3   passenger_count        float64\n",
      " 4   trip_distance          float64\n",
      " 5   RatecodeID             float64\n",
      " 6   store_and_fwd_flag     object \n",
      " 7   PULocationID           int64  \n",
      " 8   DOLocationID           int64  \n",
      " 9   payment_type           float64\n",
      " 10  fare_amount            float64\n",
      " 11  extra                  float64\n",
      " 12  mta_tax                float64\n",
      " 13  tip_amount             float64\n",
      " 14  tolls_amount           float64\n",
      " 15  improvement_surcharge  float64\n",
      " 16  total_amount           float64\n",
      " 17  congestion_surcharge   float64\n",
      "dtypes: float64(13), int64(2), object(3)\n",
      "memory usage: 13.6+ GB\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T09:29:08.911527Z",
     "start_time": "2025-08-11T09:27:54.419777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Wir wandeln den Dataframe in ein Parquet-File um\n",
    "yellow_trips_df.to_parquet('NYTaxi-TripData/yellow_tripdata.parquet', engine='fastparquet')"
   ],
   "id": "f1026914d7210bac",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Einlesen des Parquet-Files in ein Dataframe\n",
    "pf = ParquetFile('NYTaxi-TripData/yellow_tripdata.parquet')\n",
    "df = pf.to_pandas()\n",
    "# Beachte die verkürzte Zeit beim Einlesen. Ca 2,5 Minuten bei CSV-Files und ca 0,5 Minuten bei Parquet-File.\n",
    "# (Abhängig von der Leistungsfähigkeit des Computers)"
   ],
   "id": "ab55ee983e7f8631",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "id": "ee883dc286b9a52e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Fast fertig...\n",
    "#### Unser Daten-Setup und Datensicherheit\n",
    "Wir haben jetzt ein robustes Setup: Die originalen CSV-Dateien dienen als Backup, und die neu erstellte Parquet-Datei ist unsere effiziente Arbeitsgrundlage. Bevor wir mit der Datenbereinigung und -analyse beginnen, erstellen wir von unserem DataFrame eine Arbeitskopie. Das ist ein wichtiger Schritt, da Pandas sehr flexibel ist und unbeabsichtigte Änderungen den Datensatz schnell beschädigen könnten. So bleibt die Parquet-Datei immer intakt.\n",
    "\n",
    "#### Einbinden der Lookup-Tabelle\n",
    "Zusätzlich benötigen wir die Lookup-Tabelle, die wir ebenfalls in einen DataFrame einlesen. Da diese Datei wesentlich kleiner ist, muss sie nicht in eine Parquet-Datei umgewandelt werden. Diese Tabelle ist im Prinzip eine Referenz, die unsere Haupttabelle ergänzt. Sie verbindet beispielsweise eine `LocationID` aus den Trip-Daten mit dem passenden Zonennamen."
   ],
   "id": "681d103aad08dbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lookup_df = pd.read_csv('NYTaxi-Data/taxi+_zone_lookup.csv')\n",
    "lookup_df.head()"
   ],
   "id": "b7d761f83f4b3d90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Erstellen der Arbeitskope der Tripdaten\n",
    "ytdf = df.copy()\n",
    "ytdf.head()"
   ],
   "id": "26d0dba25688f200",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Abschluss und Ausblick\n",
    "Nachdem wir nun die Vorbereitung abgeschlossen und unser Daten-Setup perfektioniert haben, kann der Spaß beginnen.\n",
    "\n",
    "Hier ist unser Fahrplan für die nächsten Schritte:\n",
    "\n",
    "* Phase 1: Bereinigung & Vorbereitung\n",
    "Zuerst widmen wir uns der Bereinigung des großen DataFrames. Wir korrigieren die Datentypen der Spalten, insbesondere der Datums- und Zeitangaben, kümmern uns um fehlende Werte und filtern offensichtliche Fehler wie Fahrten mit 0 Dollar Preis oder 0 Minuten Dauer heraus. Ziel ist ein sauberer, zuverlässiger Datensatz.\n",
    "\n",
    "* Phase 2: Daten verknüpfen\n",
    "Sobald der große Trip-Datensatz sauber ist, verknüpfen wir ihn mit der kleinen Lookup-Tabelle. So können wir die numerischen LocationIDs den echten Zonennamen zuordnen. Das macht die Daten verständlicher und für die Analyse nutzbar.\n",
    "\n",
    "* Phase 3: Analyse & Visualisierung\n",
    "Mit den bereinigten und verknüpften Daten können wir endlich tiefer eintauchen. Wir werden die Daten analysieren, um Muster zu finden. Zum Beispiel:\n",
    "\n",
    "Wann sind die Stoßzeiten in New York?\n",
    "\n",
    "Welche Stadtteile sind die beliebtesten Abholorte?\n",
    "\n",
    "Wie lang sind die Fahrten durchschnittlich und wie hoch sind die Trinkgelder?\n",
    "\n",
    "Mit diesen Erkenntnissen können wir dann beginnen, die Daten zu visualisieren und vielleicht sogar die Zonen auf einer Karte darzustellen.\n",
    "\n",
    "Bereit für den Start?"
   ],
   "id": "19fab8cb0b17f8db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8bdf9d2610d1d3f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
